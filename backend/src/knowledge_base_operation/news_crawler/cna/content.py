import json
import random
import re
import time
from datetime import datetime
from pathlib import Path

from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from selenium_config import driver, wait

# ===== å°ˆç”¨ä¾‹å¤–ï¼šèµ·å§‹å³é€£çºŒ N å‰‡çš†å·²å­˜åœ¨ â†’ æå‰çµ‚æ­¢ =====


class EarlyStopAtStart(Exception):
    """èµ·å§‹éšæ®µé€£çºŒå¤šæ¬¡çš†ç‚ºå·²å­˜åœ¨ â†’ æå‰çµ‚æ­¢æ•´é«”æµç¨‹"""
    pass


def close_ad():
    """å˜—è©¦é—œé–‰å¯èƒ½å‡ºç¾çš„æ“‹ç‰ˆå‰å‰å½ˆçª—"""
    try:
        btn = driver.find_element(
            By.CSS_SELECTOR,
            "body > div > div.TopSection > div.fixedBottom > div.line-ad.need > svg > path"
        )
        btn.click()
        print("âœ–ï¸ é—œé–‰æ“‹äººçš„å½ˆçª—")
    except Exception:
        pass


def load_old_records(out_path: Path):
    """è®€å–èˆŠæœ‰æ–°èç´€éŒ„èˆ‡å·²å­˜åœ¨çš„ URL é›†åˆ"""
    if out_path.exists():
        with out_path.open("r", encoding="utf-8-sig") as f:
            old_records = json.load(f)
        old_url_set = {rec.get("url") for rec in old_records if "url" in rec}
    else:
        old_records = []
        old_url_set = set()
    print(f"ğŸ—‚ï¸  æª¢æ¸¬åˆ°èˆŠæœ‰æ–°è {len(old_records)} ç­†")
    return old_records, old_url_set


def collect_urls(topic_url: str, target_num: int, old_url_set: set):
    """
    è‡ªå‹•æ»¾å‹•ä¸¦è’é›†æ–°èé ç¶²å€ã€‚
    æ–°å¢å…©é“é–€æª»ï¼š
      A. èµ·å§‹é€£çºŒ 3 æ¬¡çš†å·²å­˜åœ¨ â†’ è§¸ç™¼ EarlyStopAtStart ä¾‹å¤–ï¼Œäº¤ç”±ä¸Šå±¤çµæŸæ•´é«”æµç¨‹ï¼ˆexit 100ï¼‰
      B. éèµ·å§‹é€£çºŒ 5 æ¬¡çš†å·²å­˜åœ¨ â†’ æå‰çµæŸæœ¬ä¸»é¡Œé çš„è’é›†ï¼ˆä¿ç•™å·²è’é›†çš„æ–°ç¶²å€ï¼Œæ­£å¸¸è¿”å›ï¼‰
    """
    driver.get(topic_url)
    time.sleep(1 + random.uniform(0.5, 1.0))
    close_ad()
    body = driver.find_element(By.TAG_NAME, "body")

    urls = []
    seen = set()
    current_url = driver.execute_script("return window.location.href")
    seen.add(current_url)

    # ===== é–€æª»å¸¸æ•¸ =====
    START_EXIST_STREAK_THRESHOLD = 3   # Aï¼šèµ·å§‹é€£çºŒã€Œå·²å­˜åœ¨ã€N æ¬¡ â†’ å…¨å±€åœæ­¢
    EXIST_STREAK_BREAK = 5             # Bï¼šéèµ·å§‹é€£çºŒã€Œå·²å­˜åœ¨ã€N æ¬¡ â†’ æå‰çµæŸç•¶å‰ä¸»é¡Œé 

    # ===== æ§åˆ¶æ——æ¨™/è¨ˆæ•¸ =====
    start_gate_active = True           # True ç›´åˆ°é‡åˆ°ç¬¬ä¸€å€‹ã€æ–°ç¶²å€ã€
    exist_streak_at_start = 0          # èµ·å§‹éšæ®µã€Œå·²å­˜åœ¨ã€é€£çºŒæ¬¡æ•¸
    exist_streak_after_start = 0       # éèµ·å§‹éšæ®µã€Œå·²å­˜åœ¨ã€é€£çºŒæ¬¡æ•¸
    soft_stop_topic = False            # è§¸ç™¼ B å¾Œç‚º Trueï¼ŒçµæŸæœ¬ä¸»é¡Œé çš„è’é›†

    def record_if_new(url, at_beginning: bool):
        """
        åˆ¤æ–· url æ˜¯å¦ç‚ºæ–°ç¶²å€ï¼›åŒæ™‚ç¶­è­·èµ·å§‹/éèµ·å§‹å…©ç¨®é€£çºŒå·²å­˜åœ¨é–€æª»ã€‚
        """
        nonlocal start_gate_active, exist_streak_at_start, exist_streak_after_start, soft_stop_topic

        if url not in old_url_set:
            urls.append(url)
            print("â‡¢ æ–°ç¶²å€", url)

            # ä»»ä¸€æ–°ç¶²å€å‡ºç¾å¾Œï¼Œèµ·å§‹é–€æª»æ°¸ä¹…å¤±æ•ˆï¼›ä¸¦é‡ç½®ã€Œéèµ·å§‹ã€é€£çºŒè¨ˆæ•¸
            if start_gate_active:
                start_gate_active = False
            exist_streak_after_start = 0

        else:
            print(f"â© å·²å­˜åœ¨ï¼Œè·³éï¼š{url}")

            # Aï¼šåƒ…åœ¨ã€Œå°šæœªå‡ºç¾ä»»ä½•æ–°ç¶²å€ã€ä¹‹å‰ï¼Œæ‰ç´¯è¨ˆèµ·å§‹é€£çºŒå·²å­˜åœ¨
            if at_beginning and start_gate_active:
                exist_streak_at_start += 1
                if exist_streak_at_start >= START_EXIST_STREAK_THRESHOLD:
                    msg = (f"[STOP] èµ·å§‹å³é€£çºŒ {exist_streak_at_start} å‰‡çš†ç‚ºå·²å­˜åœ¨ â†’ "
                           f"åˆ¤å®šæœ¬è¼ªç„¡æ–°å…§å®¹ï¼Œåœæ­¢çˆ¬èŸ²ã€‚")
                    print(msg)
                    raise EarlyStopAtStart(msg)
            else:
                # Bï¼šéèµ·å§‹ï¼ˆå·²å‡ºç¾éè‡³å°‘ä¸€å€‹æ–°ç¶²å€ï¼‰æ™‚ï¼Œé€£çºŒå·²å­˜åœ¨è¨ˆæ•¸
                if not start_gate_active:
                    exist_streak_after_start += 1
                    if exist_streak_after_start >= EXIST_STREAK_BREAK:
                        print(f"â¡ï¸ éèµ·å§‹éšæ®µé€£çºŒ {exist_streak_after_start} æ¬¡çš†ç‚ºå·²å­˜åœ¨ â†’ "
                              f"æå‰çµæŸæœ¬ä¸»é¡Œé çš„è’é›†ï¼ˆå·²æŠ“åˆ°æ–°ç¶²å€ {len(urls)} ç­†ï¼‰ã€‚")
                        soft_stop_topic = True

    # èµ·å§‹ç¬¬ä¸€ç­†ï¼ˆç•¶å‰ç¶²å€ï¼‰è¦è¨ˆå…¥èµ·å§‹é–€æª»
    record_if_new(current_url, at_beginning=True)
    last_time = time.time()

    while len(urls) < target_num and not soft_stop_topic:
        body.send_keys(Keys.PAGE_DOWN)
        time.sleep(random.uniform(0.3, 0.5))
        new_url = driver.execute_script("return window.location.href")
        if new_url != current_url and new_url not in seen:
            seen.add(new_url)
            current_url = new_url
            last_time = time.time()

            # è‹¥ä»åœ¨èµ·å§‹éšæ®µï¼Œé€™ç­†è¦ç¹¼çºŒç®—ã€Œèµ·å§‹é–€æª»ã€ï¼›å¦å‰‡ç®—ã€Œéèµ·å§‹ã€
            record_if_new(new_url, at_beginning=start_gate_active)

        if soft_stop_topic:
            break

        if time.time() - last_time >= 10:
            print("âš ï¸ 10 ç§’å…§ç„¡æ–°ç¶²å€ï¼Œåœæ­¢æ»¾å‹•ã€‚")
            break

    print(f"âœ… æœ¬æ¬¡å…±æ”¶éŒ„ {len(urls)} å‰‡æ–°èã€‚")
    return urls


def fetch_article_record(url: str):
    """çˆ¬å–å–®ç¯‡æ–°èè³‡æ–™ï¼Œå›å‚³ record dict"""
    driver.get(url)
    time.sleep(1 + random.uniform(0.5, 1.0))
    close_ad()

    try:
        title = wait.until(
            lambda d: d.find_element(By.CSS_SELECTOR, "h1 span")
        ).text.strip()
    except Exception:
        title = "æ¨™é¡Œæœªå–å¾—"

    re_date = re.compile(r"/news/[^/]+/(\d{12})\.aspx")
    m = re_date.search(url)
    date = (
        datetime.strptime(m.group(1)[:8], "%Y%m%d").strftime("%Y-%m-%d")
        if m else "æ—¥æœŸæœªå–å¾—"
    )

    cat_elems = driver.find_elements(By.CSS_SELECTOR, ".breadcrumb a")
    category = cat_elems[1].text.strip() if len(cat_elems) > 1 else ""

    paras = driver.find_elements(
        By.CSS_SELECTOR, "#article-body p, div.paragraph p")
    content = "\n".join(
        p.text.strip()
        for p in paras
        if p.text.strip() and "æœ¬ç¶²ç«™ä¹‹æ–‡å­—" not in p.text
    )

    return {
        "url": url,
        "date": date,
        "publisher": "CNA",
        "category": category,
        "title": title,
        "content": content,
        "label": True,
    }


def save_records(old_records: list, new_records: list, out_path: Path):
    """åˆä½µèˆŠæœ‰èˆ‡æ–°å¢è¨˜éŒ„ä¸¦å¯«å…¥ JSON æª”æ¡ˆ"""
    final = old_records + new_records
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8-sig") as f:
        json.dump(final, f, ensure_ascii=False, indent=2)
    print(
        f"ğŸ”¸ ç´¯ç©ç¸½æ•¸ (èˆŠæœ‰ {len(old_records)} ç­†) + (æ–°å¢ {len(new_records)} ç­†) å…± {len(final)} å‰‡ï¼Œ"
        f"å·²é€²è¡Œæ›´æ–°ä¸¦å¯«å…¥ {out_path.resolve()}"
    )
    return final


def fetch_cna_content(topic_url: str, target_num: int = 300, out_path: str = "data/raw/news/cna/cna_news.json"):
    """ä¸»æµç¨‹ï¼šå‘¼å«å„å‰¯ç¨‹å¼å®Œæˆæ–°èçˆ¬å–åŠå­˜æª”"""
    OUT_PATH = Path(out_path)
    old_records, old_url_set = load_old_records(OUT_PATH)
    try:
        urls = collect_urls(topic_url, target_num, old_url_set)
    except EarlyStopAtStart:
        # Aï¼šå…¨å±€åœæ­¢
        try:
            driver.quit()
        finally:
            print("[EXIT 100] content.py å› ã€èµ·å§‹å³é€£çºŒ 3 å‰‡çš†å·²å­˜åœ¨ã€æå‰çµæŸã€‚")
            import sys
            sys.exit(100)

    # B æƒ…æ³ä¸‹ï¼ˆæˆ–ä¸€èˆ¬æƒ…æ³ï¼‰ï¼Œæ­¤è™•æ­£å¸¸è™•ç†å·²è’é›†åˆ°çš„æ–°ç¶²å€
    new_records = []
    for url in urls:
        record = fetch_article_record(url)
        new_records.append(record)

    print(f"âš ï¸  èˆ‡èˆŠæœ‰æ–°èæ¯”å°å¾Œï¼Œæ–°å¢ {len(new_records)} ç­†æ–°çš„æ–°èã€‚")
    final = save_records(old_records, new_records, OUT_PATH)
    driver.quit()
    return final


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1].startswith("http"):
        # CLI ç›´æ¥çµ¦ä¸»é¡Œé ç¶²å€ï¼ˆæ”¯æ´ pipeline æ‰¹æ¬¡åŸ·è¡Œï¼‰
        topic_url = sys.argv[1]
        try:
            fetch_cna_content(topic_url)
        except SystemExit:
            # ä¿ç•™ exit codeï¼ˆä¾‹å¦‚ 100ï¼šèµ·å§‹å³é€£çºŒ 3 å‰‡å·²å­˜åœ¨ï¼‰
            raise
        except Exception as e:
            print(f"[ERR] content.py ç™¼ç”Ÿç•°å¸¸ï¼š{e}")
        finally:
            try:
                driver.quit()
            except Exception as e:
                print(f"[WARN] driver.quit() ç•°å¸¸ï¼š{e}")
            print("ã€content.py çµæŸã€‘")
    else:
        # é è¨­æ¸¬è©¦ç¶²å€
        test_url = "https://www.cna.com.tw/news/aipl/202507300286.aspx?topic=4782"
        fetch_cna_content(test_url)
