"""
Pipelineï¼š
1) åŸ·è¡Œ merge.py  â†’ è¼¸å‡º data/processed/news_merge/cna_ey_moi_pts.json
2) åŸ·è¡Œ dedup.py  â†’ è¼¸å‡º data/processed/news_merge/cna_ey_moi_pts_DEDUP.json
3) åŸ·è¡Œ upload_storage.py â†’ è®€å– *_DEDUP.json ä¸Šå‚³è‡³ RAG storageï¼Œ
   ä¸¦æ–¼ data/processed/news_merge/rag_storage_id å¯«å…¥ã€Œyyyy-mm-dd-hhmmã€å‘½åçš„ id æª”
4) åƒ…ç•¶ã€Œç¬¬ 3 æ­¥ç¢ºå¯¦æˆåŠŸä¸”æ–°å¢äº† *æ–°çš„* id æª”ã€æ™‚ï¼Œæ‰åŸ·è¡Œ delete_storage.py åˆªé™¤èˆŠå‘é‡åº«

èªªæ˜ï¼š
- æœ¬ç‰ˆå°ã€Œæ˜¯å¦çœŸçš„ä¸Šå‚³æˆåŠŸã€åšé›™ä¿éšªåˆ¤å®šï¼š
  (A) ä¸Šå‚³è…³æœ¬ returncode==0
  (B) ä¸Šå‚³å‰å¾Œæ¯”è¼ƒ rag_storage_id ç›®éŒ„ï¼Œå¿…é ˆå‡ºç¾ *æ–°çš„* id æª”ï¼ˆä¾ mtime èˆ‡æª”åï¼‰
- è‹¥ä»»ä¸€æ¢ä»¶ä¸æˆç«‹ï¼Œä¸€å¾‹ä¸åŸ·è¡Œåˆªé™¤ï¼Œä»¥é¿å…ã€Œç©ºå‘é‡åº«ã€èª¤åˆªæˆ–éŒ¯åˆªã€‚
"""

from __future__ import annotations

import subprocess
import sys
from pathlib import Path
from typing import Optional, Tuple, List
from dotenv import load_dotenv

load_dotenv()  # è®“ GPT_API / OPENAI_API_KEY ç­‰å¯å¾ .env è¼‰å…¥

# â”€â”€ è…³æœ¬è·¯å¾‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MERGE_SCRIPT = Path("src/knowledge_base_operation/news_merge/merge.py")
DEDUP_SCRIPT = Path("src/knowledge_base_operation/news_merge/dedup.py")
UPLOAD_SCRIPT = Path(
    "src/knowledge_base_operation/news_merge/upload_storage.py")
DELETE_SCRIPT = Path(
    "src/knowledge_base_operation/news_merge/delete_storage.py")

# â”€â”€ I/O æª”æ¡ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INPUT_FILES: List[Path] = [
    Path("data/raw/news/cna/cna_news.json"),
    Path("data/raw/news/ey/ey_news.json"),
    Path("data/raw/news/moi/moi_news.json"),
    Path("data/raw/news/pts/pts_news.json"),
]
OUTPUT_JSON = Path("data/processed/news_merge/cna_ey_moi_pts.json")
OUTPUT_DEDUP_JSON = Path("data/processed/news_merge/cna_ey_moi_pts_DEDUP.json")

# RAG storage ä¸Šå‚³ç´€éŒ„ï¼ˆç”± upload_storage.py ç”¢ç”Ÿï¼‰
RAG_ID_DIR = Path("data/processed/news_merge/rag_storage_id")


# â”€â”€ å…¬ç”¨å‡½å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _latest_file(directory: Path) -> Optional[Path]:
    if not directory.exists():
        return None
    files = [p for p in directory.iterdir() if p.is_file()]
    if not files:
        return None
    return max(files, key=lambda p: p.stat().st_mtime)


def _latest_file_snapshot(directory: Path) -> Tuple[Optional[Path], float]:
    p = _latest_file(directory)
    return (p, p.stat().st_mtime if p else 0.0)


def _newer_file_appeared(before: Tuple[Optional[Path], float],
                         after: Tuple[Optional[Path], float]) -> bool:
    """åˆ¤æ–·ä¸Šå‚³å‰å¾Œæ˜¯å¦å‡ºç¾äº† *æ–°çš„* id æª”ï¼ˆæª”æ¡ˆä¸åŒæˆ– mtime è®Šæ–°ï¼‰ã€‚"""
    p_before, t_before = before
    p_after,  t_after = after
    if p_after is None:
        return False
    if p_before is None:
        return True
    if p_after != p_before:
        return True
    return t_after > t_before


# â”€â”€ ä»»å‹™æ˜¯å¦éœ€è¦é‡è·‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def need_merge(force: bool) -> bool:
    if force or not OUTPUT_JSON.exists():
        return True
    out_m = OUTPUT_JSON.stat().st_mtime
    return any(s.exists() and s.stat().st_mtime > out_m for s in INPUT_FILES)


def need_dedup(force: bool) -> bool:
    if force or not OUTPUT_DEDUP_JSON.exists():
        return True
    return OUTPUT_JSON.exists() and OUTPUT_JSON.stat().st_mtime > OUTPUT_DEDUP_JSON.stat().st_mtime


def _latest_file_mtime(directory: Path) -> Optional[float]:
    p = _latest_file(directory)
    return p.stat().st_mtime if p else None


def need_upload(force: bool) -> bool:
    """
    ä¸Šå‚³æ¢ä»¶ï¼š
    - å¼·åˆ¶ä¸Šå‚³ï¼Œæˆ–
    - æ²’æœ‰ä»»ä½• id æª”ï¼Œæˆ–
    - dedup è¼¸å‡ºæ¯”ç›®å‰æœ€æ–° id æª”é‚„æ–°ï¼ˆä»£è¡¨å…§å®¹æ›´æ–°ï¼‰
    """
    if force:
        return True
    if not OUTPUT_DEDUP_JSON.exists():
        return False
    latest_id_mtime = _latest_file_mtime(RAG_ID_DIR)
    if latest_id_mtime is None:
        return True
    return OUTPUT_DEDUP_JSON.stat().st_mtime > latest_id_mtime


# â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    import argparse
    ap = argparse.ArgumentParser(
        description="News merge + dedup + upload-to-RAG (+conditional delete) pipeline"
    )
    ap.add_argument("--force-merge",  action="store_true", help="å¼·åˆ¶é‡è·‘åˆä½µ")
    ap.add_argument("--force-dedup",  action="store_true", help="å¼·åˆ¶é‡è·‘å»é‡")
    ap.add_argument("--force-upload", action="store_true",
                    help="å¼·åˆ¶é‡è·‘ä¸Šå‚³ RAG storage")
    # å‚³éçµ¦ upload/delete çš„é¡å¤–åƒæ•¸ï¼ˆå¯é¸ï¼‰
    ap.add_argument("--upload-args", default="",
                    help="å‚³çµ¦ upload_storage.py çš„å…¶ä»–åƒæ•¸å­—ä¸²")
    ap.add_argument("--delete-args", default="",
                    help="å‚³çµ¦ delete_storage.py çš„å…¶ä»–åƒæ•¸å­—ä¸²")
    args = ap.parse_args()

    # 1) åˆä½µ
    if need_merge(args.force_merge):
        print("â–¶ åˆä½µï¼šåŸ·è¡Œ merge.py")
        r = subprocess.run([sys.executable, str(MERGE_SCRIPT)], check=False)
        if r.returncode != 0:
            sys.exit(r.returncode)
    else:
        print("âœ“ åˆä½µï¼šå·²æ˜¯æœ€æ–°ï¼Œè·³é")

    # 2) å»é‡
    if need_dedup(args.force_dedup):
        print("â–¶ å»é‡ï¼šåŸ·è¡Œ dedup.py")
        r = subprocess.run([sys.executable, str(DEDUP_SCRIPT)], check=False)
        if r.returncode != 0:
            sys.exit(r.returncode)
    else:
        print("âœ“ å»é‡ï¼šå·²æ˜¯æœ€æ–°ï¼Œè·³é")

    # 3) ä¸Šå‚³ RAG storageï¼ˆé›™ä¿éšªåˆ¤å®šï¼šreturncode + æ–° id æª”ï¼‰
    uploaded_this_run = False
    if need_upload(args.force_upload):
        before_snap = _latest_file_snapshot(RAG_ID_DIR)
        print("â–¶ ä¸Šå‚³ï¼šåŸ·è¡Œ upload_storage.py")
        cmd = [sys.executable, str(UPLOAD_SCRIPT)]
        if args.upload_args.strip():
            # å…è¨±æŠŠä¸€æ•´æ®µåƒæ•¸å­—ä¸²å‚³çµ¦å­ç¨‹å¼ï¼ˆä¾‹å¦‚ --name è‡ªè¨‚ï¼‰
            cmd.extend(args.upload_args.split())
        r = subprocess.run(cmd, check=False)
        if r.returncode != 0:
            # upload_storage.py è¨­è¨ˆï¼šç´¢å¼•å¤±æ•—æœƒå›å‚³é 0ï¼Œç›´æ¥ä¸­æ­¢
            sys.exit(r.returncode)
        after_snap = _latest_file_snapshot(RAG_ID_DIR)
        uploaded_this_run = _newer_file_appeared(before_snap, after_snap)
        if not uploaded_this_run:
            # æ¥µå°‘æ•¸æƒ…æ³ï¼ˆä¾‹å¦‚é‡è¤‡å…§å®¹ï¼‰ï¼Œä¿å®ˆè™•ç†ï¼šè¦–ç‚ºæœªæˆåŠŸï¼Œä¸åšåˆªé™¤
            print("âš  ä¸Šå‚³è…³æœ¬æˆåŠŸä½†æœªåµæ¸¬åˆ°æ–° id æª”ï¼Œå°‡è·³éåˆªé™¤èˆŠå‘é‡åº«ã€‚")
    else:
        print("âœ“ ä¸Šå‚³ï¼šå·²æ˜¯æœ€æ–°ï¼Œè·³é")

    # 4) åƒ…ç•¶ã€Œæœ¬æ¬¡ç¢ºå¯¦æ–°å¢å‘é‡åº«ã€æ‰æ¸…ç†èˆŠå‘é‡åº«
    if uploaded_this_run:
        print("â–¶ æ¸…ç†ï¼šåŸ·è¡Œ delete_storage.pyï¼ˆåƒ…ä¿ç•™æœ€æ–° RAG å‘é‡åº«ï¼‰")
        cmd = [sys.executable, str(DELETE_SCRIPT)]
        if args.delete_args.strip():
            cmd.extend(args.delete_args.split())
        r = subprocess.run(cmd, check=False)
        if r.returncode != 0:
            sys.exit(r.returncode)
    else:
        print("âœ“ æ¸…ç†ï¼šæœ¬æ¬¡æœªæ–°å¢å‘é‡åº«ï¼Œè·³é delete_storage.py")

    print("âœ… Pipeline å®Œæˆ â†’", OUTPUT_DEDUP_JSON)
    if RAG_ID_DIR.exists():
        print("ğŸ†” æœ€æ–° RAG id æª”ç›®éŒ„ï¼š", RAG_ID_DIR)


if __name__ == "__main__":
    main()
