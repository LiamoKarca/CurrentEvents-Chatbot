"""
MOI æ–°èçˆ¬èŸ²ï¼ˆå…¨é‡ + å‹•æ…‹åµæ¸¬ä¸Šé™é æ•¸ + é€ç¯‡å­˜æª” + é€£ä¸‰é‡è¤‡å³ä¸­æ–· + èµ·å§‹é é˜²å + é€£5é 0ç¯‡è¦–ç‚ºç¶­è­· + å¯æŒ‡å®šèµ·å§‹é ï¼‰
----------------------------------------------------------------
é‡é»ï¼š
1) æŒ‡å®šèµ·å§‹é ï¼špython src/knowledge_base_operation/news_crawler/moi/moi.py --start-page <PAGE>
2) å‹•æ…‹åµæ¸¬ä¸Šé™é æ•¸ï¼›å¤±æ•—å›é€€ç‚º 1500ã€‚
3) é€ç¯‡å³æ™‚å­˜æª”ï¼ˆåŸå­å¯«å…¥ï¼‰ã€‚
4) é€£çºŒä¸‰ç¯‡çš†ç‚ºé‡è¤‡ â†’ ä¸­æ–·ï¼ˆåƒ…éèµ·å§‹é ç”Ÿæ•ˆï¼‰ã€‚
5) èµ·å§‹é é˜²åï¼šèµ·å§‹é ä¸å¥—ç”¨ä¸­æ–·é–€æª»ï¼Œä¸”èµ·å§‹é çµæŸæ™‚é‡ç½®é€£è™Ÿã€‚
6) è‹¥é€£çºŒ 5 é åˆ—è¡¨çš†é¡¯ç¤º 0 ç¯‡ â†’ ç ”åˆ¤ç«™æ–¹ç¶­è­·ä¸­ â†’ ç«‹å³çµ‚æ­¢ï¼ˆä¸¦ä¿éšªå­˜æª”ä¸€æ¬¡ï¼‰ã€‚
è¼¸å‡ºæª”ï¼šbackend/data/raw/news/moi.json
"""

from __future__ import annotations

import argparse
import json
import os
import random
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

# =========================
# å…¨åŸŸå¸¸æ•¸è¨­å®š
# =========================
BASE_URL = "https://www.moi.gov.tw/News.aspx?n=4&sms=9009&page="
PAGE_SIZE = 15  # åˆ—è¡¨æ¯é ç­†æ•¸
OUTPUT_PATH = Path("backend/data/raw/news/moi.json")  # å–®ä¸€è¼¸å‡ºæª”æ¡ˆ
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
)

# XPathï¼ˆSelenium ä¸èƒ½ä½¿ç”¨ /text()ï¼‰
XPATH_CANDIDATES = [
    '//*[@id="CCMS_Content"]/div/div/div/div[3]/div/div/div/ul[2]/li[2]/span',
    (
        "/html/body/form/div[3]/div/div/div/div[3]/div/div/div/div[2]/div/div/div/"
        "div/div/div/div/div[2]/div/div/div/div[3]/div/div/div/div[3]/div/div/div/"
        "ul[2]/li[2]/span"
    ),
]

FALLBACK_MAX_PAGE = 1500                 # å‹•æ…‹é æ•¸è§£æå¤±æ•—æ™‚å›é€€
MAX_CONSEC_DUP = 3                       # é€£çºŒé‡è¤‡é–€æª»
SUPPRESS_DUP_STOP_ON_START_PAGE = True   # èµ·å§‹é æ˜¯å¦é—œé–‰é–€æª»
ZERO_PAGE_LIMIT = 5                      # â˜… é€£çºŒ 0 ç¯‡é æ•¸é”æ­¤ä¸Šé™ â†’ è¦–ç‚ºç¶­è­·ä¸­

# =========================
# å·¥å…·å‡½å¼
# =========================


def init_driver() -> webdriver.Chrome:
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--start-maximized")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument(f"user-agent={USER_AGENT}")

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=chrome_options,
    )
    return driver


def load_existing_data(path: Path = OUTPUT_PATH) -> Tuple[List[Dict], set]:
    if path.exists():
        with path.open("r", encoding="utf-8") as f:
            old_data = json.load(f)
    else:
        old_data = []
    old_titles = {item.get("title", "")
                  for item in old_data if "title" in item}
    return old_data, old_titles


def atomic_write_json(data: List[Dict], path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = path.with_suffix(path.suffix + ".tmp")
    with tmp_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp_path, path)


def page_url(page_index: int) -> str:
    return f"{BASE_URL}{page_index}&PageSize={PAGE_SIZE}"


def parse_news_links(html: str) -> List[str]:
    """
    å›å‚³å…§é ç›¸å°é€£çµæ¸…å–®ã€‚ç¶­è­·é é¢é€šå¸¸ä¸å«é æœŸçš„è¡¨æ ¼çµæ§‹ï¼Œå°‡è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")
    rows = soup.find_all("tr")
    if not rows:
        return []
    rows = rows[1:]  # ç§»é™¤è¡¨é ­

    hrefs: List[str] = []
    for row in rows:
        a_tag = row.find("a", class_="aspx")
        href = a_tag.get("href") if a_tag else None
        if href and not href.lower().startswith("javascript:"):
            hrefs.append(href)
    return hrefs


def detect_max_page(driver: webdriver.Chrome, timeout_sec: int = 10) -> tuple[int, str]:
    driver.get(page_url(1))
    wait = WebDriverWait(driver, timeout_sec)

    # 1) ä¸»è¦ XPath
    for xp in XPATH_CANDIDATES:
        try:
            elem = wait.until(EC.presence_of_element_located((By.XPATH, xp)))
            text = (elem.text or "").strip()
            nums = re.findall(r"\d+", text)
            if nums:
                max_page = int(nums[-1])
                return max_page, f'XPath å‘½ä¸­ï¼š{xp} | span.text="{text}"'
        except Exception:
            continue

    # 2) å‚™æ´ï¼šæƒæ ul[2] li
    try:
        ul_xpath = '//*[@id="CCMS_Content"]/div/div/div/div[3]/div/div/div/ul[2]'
        ul_elem = driver.find_element(By.XPATH, ul_xpath)
        li_elems = ul_elem.find_elements(By.TAG_NAME, "li")
        bucket: List[int] = []
        preview_texts: List[str] = []
        for li in li_elems:
            t = (li.text or "").strip()
            if len(preview_texts) < 5:
                preview_texts.append(t)
            bucket.extend(int(n) for n in re.findall(r"\d+", t))
        if bucket:
            mp = max(bucket)
            return mp, f"UL[2] æƒæ | å–æœ€å¤§æ•¸å­—={mp} | ç¯„ä¾‹æ–‡å­—={preview_texts}"
    except Exception:
        pass

    # 3) å‚™æ´ï¼šå°‹æ‰¾ã€Œæœ€å¾Œã€é€£çµï¼Œå¾ href è§£æ page=
    try:
        last_link = driver.find_element(By.PARTIAL_LINK_TEXT, "æœ€å¾Œ")
        href = last_link.get_attribute("href") or ""
        m = re.search(r"page=(\d+)", href)
        if m:
            mp = int(m.group(1))
            return mp, f'é€£çµã€Œæœ€å¾Œã€å‘½ä¸­ | href="{href}" â†’ page={mp}'
    except Exception:
        pass

    # 4) å…¨éƒ¨å¤±æ•—ï¼Œå›é€€
    return FALLBACK_MAX_PAGE, f"âš ï¸ è§£æå¤±æ•—ï¼Œå›é€€ç‚º {FALLBACK_MAX_PAGE}ï¼ˆè«‹æª¢æŸ¥ DOM æˆ–æ›´æ–° XPathï¼‰"


def parse_news_page(driver: webdriver.Chrome, relative_url: str) -> Optional[Dict]:
    full_url = f"https://www.moi.gov.tw/{relative_url}"
    driver.get(full_url)
    time.sleep(random.uniform(1.5, 3.0))

    soup = BeautifulSoup(driver.page_source, "html.parser")

    title_text: Optional[str] = None
    title_block = soup.find("div", class_="simple-text title")
    if title_block:
        inner = title_block.find("div", class_="in")
        if inner:
            title_text = inner.get_text("h3")
    if not title_text:
        h3 = soup.find("h3")
        if h3 and h3.text:
            title_text = h3.get_text()
    if not title_text:
        return None

    title = title_text.replace(" ", "").replace("ã€€", "")

    date_str = ""
    info_block = soup.find("div", class_="list-text detail")
    if info_block:
        li = info_block.find("li")
        if li and li.text:
            date_str = li.text.strip().replace("ç™¼å¸ƒæ—¥æœŸï¼š", "").split(" ")[0]

    paragraphs = soup.select("div.p p")
    fragments: List[str] = []
    for p in paragraphs:
        txt = p.get_text(strip=True)
        if "å ±å°" in txt or "æ–°èä¾†æº" in txt:
            continue
        fragments.append(txt)

    for sym in ["\n", '"', "ã€Œ", "ã€", "ï¼š", "ï¼›", ":", ";", ",", "{", "}", "[", "]"]:
        fragments = [t.replace(sym, "") for t in fragments]

    raw = "".join(fragments)
    parts = [seg for seg in raw.split("ã€‚") if seg.strip()]
    content = "\n".join(seg + "ã€‚" for seg in parts) if parts else ""

    return {
        "date": date_str,
        "publisher": "MOI",
        "category": "æ”¿åºœ",
        "title": title,
        "content": content,
        "label": True,
        "url": full_url,
    }


# =========================
# ä¸»æµç¨‹
# =========================
def crawl_moi_news(start_page: int = 1) -> None:
    start_time = datetime.now()
    print(f"é–‹å§‹æ™‚é–“: {start_time:%Y-%m-%d %H:%M:%S}")

    driver = init_driver()
    old_data, old_titles = load_existing_data(OUTPUT_PATH)

    consecutive_dup = 0
    consecutive_zero_pages = 0   # â˜… æ–°å¢ï¼šé€£çºŒ 0 ç¯‡é é¢è¨ˆæ•¸
    stop_requested = False

    try:
        max_page, proof = detect_max_page(driver, timeout_sec=10)
        print(f"ğŸ” å·²å¯¦éš›è®€å–åˆ°çš„é æ•¸ä¸Šé™ï¼š{max_page}")
        print(f"   â””â”€ ä¾†æºè­‰æ“šï¼š{proof}")
        if "å›é€€" in proof or "è§£æå¤±æ•—" in proof:
            print("âš ï¸ æ³¨æ„ï¼šç›®å‰ä½¿ç”¨ä¿å®ˆå›é€€å€¼ã€‚å»ºè­°æª¢æŸ¥åˆ†é  DOM èˆ‡ XPathã€‚")

        if start_page < 1:
            print(f"âš ï¸ èµ·å§‹é  {start_page} éæ³•ï¼Œå·²æ›´æ­£ç‚º 1ã€‚")
            start_page = 1
        if start_page > max_page:
            print(f"âš ï¸ èµ·å§‹é  {start_page} è¶…éä¸Šé™ {max_page}ï¼Œä¸é€²è¡Œçˆ¬å–ã€‚")
            return

        print(f"â–¶ï¸  å°‡å¾ç¬¬ {start_page} é é–‹å§‹çˆ¬å–ï¼Œç›´åˆ°ç¬¬ {max_page} é ã€‚")

        for page_index in range(start_page, max_page + 1):
            if stop_requested:
                break

            # èµ·å§‹é çš„é–€æª»é—œé–‰é–‹é—œï¼ˆåƒ…è©²é ï¼‰
            dup_gate_enabled = not (
                SUPPRESS_DUP_STOP_ON_START_PAGE and page_index == start_page
            )

            url = page_url(page_index)
            print(
                f"ğŸ“„ æ­£åœ¨è™•ç†åˆ—è¡¨é ï¼š{url}ï¼ˆèµ·å§‹é é–€æª» {'é–‹å•Ÿ' if dup_gate_enabled else 'é—œé–‰'}ï¼‰"
            )
            driver.get(url)
            time.sleep(3)

            hrefs = parse_news_links(driver.page_source)
            print(f"ğŸ“‘ æœ¬é å…± {len(hrefs)} ç¯‡")

            # â˜… ç¶­è­·åµæ¸¬ï¼šè‹¥æœ¬é  0 ç¯‡ â†’ ç´¯è¨ˆï¼›å¦å‰‡é‡ç½®
            if len(hrefs) == 0:
                consecutive_zero_pages += 1
                print(
                    f"ğŸ§° ç¶­è­·åµæ¸¬ï¼šé€£çºŒ 0 ç¯‡é æ•¸ {consecutive_zero_pages}/{ZERO_PAGE_LIMIT}"
                )
                if consecutive_zero_pages >= ZERO_PAGE_LIMIT:
                    # ä¿éšªå­˜æª”ä¸€æ¬¡ï¼ˆé€ç¯‡å·²å¯«ï¼Œä½†å†ç¢ºä¿ï¼‰
                    atomic_write_json(old_data, OUTPUT_PATH)
                    print("ğŸ›‘ åµæ¸¬åˆ°é€£çºŒ 5 é  0 ç¯‡ï¼Œç ”åˆ¤ã€å…§æ”¿éƒ¨å…¨çƒè³‡è¨Šç¶² ç³»çµ±ç¶­è­·ä¸­ã€ï¼Œæå‰çµ‚æ­¢ã€‚")
                    stop_requested = True
                    break
                # 0 ç¯‡é é¢æ²’æœ‰æ–‡ç« å¯è™•ç†ï¼Œç›´æ¥é€²å…¥ä¸‹ä¸€é 
                print(f"â¡ï¸ é é¢é€²åº¦ï¼š{page_index}/{max_page}")
                continue
            else:
                # åªè¦é‡åˆ°æœ‰æ–‡ç« çš„é é¢ï¼Œå³é‡ç½®ç¶­è­·è¨ˆæ•¸
                if consecutive_zero_pages > 0:
                    print("ğŸ”„ åµæ¸¬åˆ°é 0 ç¯‡é é¢ï¼Œç¶­è­·è¨ˆæ•¸å·²é‡ç½®ç‚º 0ã€‚")
                consecutive_zero_pages = 0

            for i, rel in enumerate(hrefs, start=1):
                if stop_requested:
                    break

                try:
                    article = parse_news_page(driver, rel)

                    if not article:
                        print(f"âš ï¸ å…§é è§£æå¤±æ•—ï¼Œè¦–ç‚ºéé‡è¤‡äº‹ä»¶ï¼Œé‡ç½®é€£è™Ÿï¼›ç•¥éï¼š{rel}")
                        consecutive_dup = 0
                        continue

                    title = article["title"]

                    if title in old_titles:
                        consecutive_dup += 1
                        print(
                            f"â­ è·³éé‡è¤‡æ–°èï¼ˆé€£è™Ÿ {consecutive_dup}/{MAX_CONSEC_DUP}ï¼›"
                            f"èµ·å§‹é é–€æª»{'é–‹å•Ÿ' if dup_gate_enabled else 'é—œé–‰'}ï¼‰ï¼š{title}"
                        )
                        if consecutive_dup >= MAX_CONSEC_DUP:
                            if dup_gate_enabled:
                                atomic_write_json(old_data, OUTPUT_PATH)
                                print("ğŸ›‘ é€£çºŒä¸‰ç¯‡é‡è¤‡ï¼Œè§¸ç™¼ä¸­æ–·ä¸¦å·²ä¿éšªå­˜æª”ã€‚")
                                stop_requested = True
                            else:
                                print("â›³ èµ·å§‹é é–€æª»é—œé–‰ï¼šå³ä½¿é€£ä¸‰é‡è¤‡äº¦ä¸ä¸­æ–·ã€‚")
                        continue
                    else:
                        consecutive_dup = 0
                        old_data.append(article)
                        old_titles.add(title)
                        atomic_write_json(old_data, OUTPUT_PATH)
                        print(f"âœ… æ–°å¢ä¸¦å­˜æª”ï¼š{title}  ({i}/{len(hrefs)})")

                except Exception as exc:
                    print(f"âš ï¸ å–®ç¯‡è™•ç†ä¾‹å¤–ï¼ˆè¦–ç‚ºéé‡è¤‡ä¸¦é‡ç½®é€£è™Ÿï¼‰ï¼š{rel}ï¼š{exc!r}")
                    consecutive_dup = 0
                finally:
                    try:
                        driver.delete_all_cookies()
                        driver.get("about:blank")
                    except Exception:
                        pass
                    time.sleep(random.uniform(1.5, 3.0))

            # â˜… èµ·å§‹é è™•ç†å®Œç•¢ â†’ é‡ç½®é€£è™Ÿï¼Œé¿å…å¤–æº¢åˆ°ä¸‹ä¸€é 
            if page_index == start_page:
                if not dup_gate_enabled:
                    consecutive_dup = 0
                    print("ğŸ” èµ·å§‹é çµæŸï¼šå·²é‡ç½®é€£çºŒé‡è¤‡è¨ˆæ•¸ï¼ˆé–€æª»å°‡æ–¼ä¸‹ä¸€é èµ·ç”Ÿæ•ˆï¼‰ã€‚")

            print(f"â¡ï¸ é é¢é€²åº¦ï¼š{page_index}/{max_page}")

    finally:
        try:
            driver.quit()
        except Exception:
            pass

        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()
        print(f"çµæŸæ™‚é–“: {end_time:%Y-%m-%d %H:%M:%S}")
        print(f"ç¨‹å¼è€—æ™‚: {elapsed:.2f} ç§’")


# =========================
# åƒæ•¸è§£æèˆ‡é€²å…¥é»
# =========================
def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="MOI æ–°èçˆ¬èŸ²ï¼ˆå…¨é‡ + å‹•æ…‹é æ•¸åµæ¸¬ + é€ç¯‡åŸå­å¯«å…¥ + é€£ä¸‰é‡è¤‡å³ä¸­æ–· + èµ·å§‹é é˜²å + é€£5é 0ç¯‡è¦–ç‚ºç¶­è­· + å¯æŒ‡å®šèµ·å§‹é ï¼‰"
    )
    parser.add_argument(
        "--start-page",
        type=int,
        default=1,
        help="æŒ‡å®šå¾å“ªä¸€é é–‹å§‹çˆ¬å–ï¼ˆé è¨­ï¼š1ï¼‰",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    crawl_moi_news(start_page=args.start_page)
