"""
EYï¼ˆè¡Œæ”¿é™¢ï¼‰æ–°èçˆ¬èŸ²
- åˆ—è¡¨é ï¼š https://www.ey.gov.tw/Page/6485009ABEC1CB9C?page={n}&PS=200&
- å…§é ï¼šæ“·å–æ¨™é¡Œã€æ—¥æœŸï¼ˆæ°‘åœ‹â†’è¥¿å…ƒï¼‰ã€åˆ†é¡ã€ç™¼å¸ƒè€…ã€å…§å®¹ã€URL
- æ¸…ç†ï¼šç§»é™¤æ›è¡Œèˆ‡é›œè¨Šç¬¦è™Ÿï¼Œä¿ç•™ã€Œã€‚ã€ä½œç‚ºåˆ†å¥ï¼Œæœ€çµ‚å…§å®¹ä¸å«æ›è¡Œ
- å»é‡ï¼šä¾ url èˆ‡ title
- çµ‚æ­¢æ¢ä»¶ï¼šèµ·å§‹é ä¹‹å¾Œï¼Œé€£çºŒ 5 å‰‡é‡è¤‡å³çµ‚æ­¢
- å³æ™‚ä¿å­˜ï¼šæ¯æˆåŠŸæ–°å¢ä¸€ç¯‡ç«‹åˆ»åŸå­å¯«å…¥ backend/data/raw/news/ey.json
- åƒæ•¸ï¼š--start-pageï¼ˆèµ·å§‹é ï¼Œé è¨­ 1ï¼›èµ·å§‹é ä¸å¥—ç”¨é€£çºŒé‡è¤‡è¦å‰‡ï¼‰
        --max-pageï¼ˆæœ€å¤§é ï¼Œé è¨­ç¨‹å¼å¸¸æ•¸ï¼‰
- è¼¸å‡ºï¼šbackend/data/raw/news/ey.jsonï¼ˆç›¸å°è·¯å¾‘ï¼Œå¢é‡æ›´æ–°ï¼‰
- python backend/src/app/crawlers/ey/ey.py --start-page 1 --max-page 106
"""

from __future__ import annotations

import argparse
import json
import random
import time
import os

from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# -------------------------- å¸¸æ•¸è¨­å®š --------------------------

BASE_URL = "https://www.ey.gov.tw/Page/6485009ABEC1CB9C"
PAGE_SIZE = 200
MAX_PAGE = 106  # ç«™å…§è§€æ¸¬ä¸Šé™ï¼›å¯ä¾éœ€æ±‚èª¿æ•´
PUBLISHER = "EY"
CATEGORY = "æ”¿åºœ"
DUP_STOP_THRESHOLD = 5  # é€£çºŒ 5 å‰‡é‡è¤‡å³çµ‚æ­¢ï¼ˆèµ·å§‹é ä¸è¨ˆï¼‰

# æ¸…ç†å…§å®¹æ™‚è¦ç§»é™¤çš„ç¬¦è™Ÿï¼ˆä¿ç•™ä¸­æ–‡å¥è™Ÿï¼‰
SYMBOLS_TO_REMOVE = ['"', "ã€Œ", "ã€", "ï¼š",
                     "ï¼›", ":", ";", ",", "{", "}", "[", "]"]

# -------------------------- è³‡æ–™çµæ§‹ --------------------------


@dataclass
class Article:
    date: str
    publisher: str
    category: str
    title: str
    content: str
    label: bool
    url: str

# -------------------------- è·¯å¾‘/IO --------------------------


def find_project_root() -> Path:
    """
    å°ˆæ¡ˆæ ¹å®šä½ï¼š
    1) å…ˆæ¡ç”¨ PROJECT_ROOT æˆ– DATA_ROOTï¼ˆåªè¦æ±‚å…¶ä¸‹æœ‰ data/ï¼‰
    2) å†è‡ªæ­¤æª”ä¸€è·¯å¾€ä¸Šï¼Œå„ªå…ˆæŒ‘åŒæ™‚å…·æœ‰ data/ èˆ‡ å°ˆæ¡ˆæ¨™è¨˜(.git æˆ– pyproject.toml) çš„çˆ¶å±¤
    3) è‹¥ç„¡å°ˆæ¡ˆæ¨™è¨˜ï¼Œé¸ã€Œè·¯å¾‘ä¸Šæœ€ä¸Šå±¤ã€å¸¶ data/ çš„çˆ¶å±¤
    4) å…¨éƒ¨æ²’æœ‰å°±å› CWD
    """
    # 1) ç’°å¢ƒè®Šæ•¸
    env_root = os.environ.get("PROJECT_ROOT") or os.environ.get("DATA_ROOT")
    if env_root:
        root = Path(env_root).resolve()
        if (root / "data").is_dir():
            return root

    # 2) å¾€ä¸Šå°‹æ‰¾ï¼šå„ªå…ˆå¸¶ data/ + å°ˆæ¡ˆæ¨™è¨˜
    here = Path(__file__).resolve()
    topmost_with_data = None
    for parent in [here] + list(here.parents):
        has_data = (parent / "data").is_dir()
        if has_data:
            topmost_with_data = parent  # ä¸æ–·è¦†å¯«ï¼Œæœ€çµ‚æœƒæ˜¯ã€Œæœ€ä¸Šå±¤ã€å¸¶ data/ çš„çˆ¶å±¤
            has_marker = (parent / ".git").is_dir() or (parent /
                                                        "pyproject.toml").is_file()
            if has_marker:
                return parent

    # 3) ç„¡å°ˆæ¡ˆæ¨™è¨˜ï¼šé€€å›ã€Œæœ€ä¸Šå±¤ã€å¸¶ data/ çš„çˆ¶å±¤
    if topmost_with_data:
        return topmost_with_data

    # 4) å¯¦åœ¨æ‰¾ä¸åˆ°ï¼Œå› CWD
    return Path.cwd().resolve()


def output_path() -> Path:
    """
    å›ºå®šè¼¸å‡ºåˆ°ï¼šbackend/data/raw/news/ey.json
    ï¼ˆproject_root ç”± find_project_root() æ±ºå®šï¼‰
    """
    root = find_project_root()

    # å¦‚æœ root å·²ç¶“æ˜¯ backendï¼ˆä¸”åº•ä¸‹å°±æœ‰ data/ï¼‰ï¼Œå°±ä¸è¦å†å¤šåŠ ä¸€å±¤ backend
    if root.name == "backend" and (root / "data").is_dir():
        out_dir = root / "data" / "raw" / "news"
    else:
        out_dir = root / "backend" / "data" / "raw" / "news"

    out_dir.mkdir(parents=True, exist_ok=True)
    return out_dir / "ey.json"


def load_existing(filepath: Path) -> List[dict]:
    """è¼‰å…¥æ—¢æœ‰ JSON é™£åˆ—ï¼›è‹¥ç„¡æˆ–æ ¼å¼éŒ¯èª¤å‰‡å›ç©ºé™£åˆ—ã€‚"""
    if not filepath.exists():
        return []
    try:
        with filepath.open("r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, list) else []
    except json.JSONDecodeError:
        return []


def atomic_save_json(filepath: Path, data: List[dict]) -> None:
    """
    åŸå­å¯«å…¥ JSONï¼šå…ˆå¯«å…¥è‡¨æ™‚æª”å†ä»¥ replace() è¦†è“‹æ­£å¼æª”ï¼Œ
    ç¢ºä¿ä»»ä½•æ™‚åˆ»ç£ç¢Ÿä¸Šçš„æª”æ¡ˆéƒ½æ˜¯å®Œæ•´å¯è§£æçš„ã€‚
    """
    tmp = filepath.with_suffix(filepath.suffix + ".tmp")
    with tmp.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
        f.flush()
    tmp.replace(filepath)

# -------------------------- çˆ¬èŸ²æ ¸å¿ƒ --------------------------


def setup_driver() -> webdriver.Chrome:
    """å»ºç«‹ Headless Chrome WebDriverã€‚"""
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--start-maximized")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36"
    )
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)


def get_list_page_url(page_index: int) -> str:
    return f"{BASE_URL}?page={page_index}&PS={PAGE_SIZE}&"


def extract_links_from_list_html(html: str) -> List[str]:
    """å¾åˆ—è¡¨é  HTML æ“·å–æ–°èç›¸å°é€£çµã€‚"""
    soup = BeautifulSoup(html, "html.parser")
    boxes = soup.find_all("div", class_="news_box pdf_box")
    links: List[str] = []
    for box in boxes:
        tag_a = box.find("a")
        if not tag_a:
            continue
        href = tag_a.get("href", "").strip()
        if href:
            links.append(href)
    return links


def parse_roc_date(roc_text: str) -> str:
    """
    è§£ææ°‘åœ‹æ—¥æœŸå­—ä¸²ï¼Œä¾‹å¦‚ã€Œæ—¥æœŸï¼š114-07-29ã€â†’ è¥¿å…ƒã€Œ2025-07-29ã€ã€‚
    """
    text = roc_text.strip().replace("æ—¥æœŸï¼š", "").replace("æ—¥æœŸ:", "").strip()
    parts = text.split("-")
    if len(parts) != 3:
        return text
    year = int(parts[0]) + 1911
    month = int(parts[1])
    day = int(parts[2])
    return f"{year:04d}-{month:02d}-{day:02d}"


def clean_text(raw: str) -> str:
    """
    å…§å®¹æ¸…ç†ï¼š
    - å»é™¤ \n/\r èˆ‡é›œè¨Šç¬¦è™Ÿï¼ˆä¿ç•™ã€Œã€‚ã€ï¼‰
    - ä»¥ã€Œã€‚ã€åˆ†å¥ã€éæ¿¾ç©ºç™½ç‰‡æ®µä¸¦æ¥å›
    - æœ€çµ‚ä¸å«æ›è¡Œå­—å…ƒï¼›è‹¥éç©ºå­—ä¸²ï¼Œç¢ºä¿ä»¥ã€Œã€‚ã€çµå°¾
    """
    text = raw.strip().replace("\n", "").replace("\r", "")
    for sym in SYMBOLS_TO_REMOVE:
        text = text.replace(sym, "")
    parts = [p.strip() for p in text.split("ã€‚") if p.strip()]
    if not parts:
        return ""
    joined = "ã€‚".join(parts)
    if not joined.endswith("ã€‚"):
        joined += "ã€‚"
    return joined


def extract_article_from_detail_html(html: str, url: str) -> Optional[Article]:
    """å¾å…§é  HTML æ“·å–ä¸€ç¯‡ Articleï¼›è‹¥çµæ§‹ç¼ºå¤±å‰‡å›å‚³ Noneã€‚"""
    soup = BeautifulSoup(html, "html.parser")

    title_tag = soup.find("span", class_="h2")
    if not title_tag:
        return None
    title = title_tag.get_text(strip=True).replace(" ", "").replace("ã€€", "")

    time_tag = soup.find("span", class_="date_style2")
    if not time_tag:
        return None
    span = time_tag.find("span")
    if not span:
        return None
    iso_date = parse_roc_date(span.get_text())

    paragraphs = soup.select("div.words_content p")
    texts: List[str] = []
    for p in paragraphs:
        t = p.get_text(separator=" ", strip=True)
        if ("å ±å°" in t) or ("æ–°èä¾†æº" in t):
            continue
        texts.append(t)
    content = clean_text("".join(texts))

    return Article(
        date=iso_date,
        publisher=PUBLISHER,
        category=CATEGORY,
        title=title,
        content=content,
        label=True,
        url=url,
    )

# -------------------------- ä¸»æµç¨‹ --------------------------


def crawl(start_page: int, max_page: int) -> None:
    start_time = datetime.now()
    print(f"é–‹å§‹æ™‚é–“: {start_time:%Y-%m-%d %H:%M:%S}")

    out_file = output_path()
    all_articles: List[dict] = load_existing(out_file)

    # å»é‡é›†åˆï¼ˆå¾æ—¢æœ‰è³‡æ–™åˆå§‹åŒ–ï¼‰
    seen_urls = {item.get("url", "") for item in all_articles if "url" in item}
    seen_titles = {item.get("title", "")
                   for item in all_articles if "title" in item}

    # é€£çºŒé‡è¤‡è¨ˆæ•¸ï¼ˆèµ·å§‹é ä¸è¨ˆï¼‰
    dup_streak = 0

    driver = setup_driver()

    try:
        for page_idx in range(start_page, max_page + 1):
            list_url = get_list_page_url(page_idx)
            print(f"ğŸ“„ åˆ—è¡¨é ï¼š{list_url}")

            driver.get(list_url)
            time.sleep(random.uniform(2.0, 4.0))

            links = extract_links_from_list_html(driver.page_source)
            print(f"  â†³ æœ¬é å…± {len(links)} å‰‡")

            for idx, rel in enumerate(links, start=1):
                full_url = f"https://www.ey.gov.tw{rel}"

                # URL å»é‡
                if full_url in seen_urls:
                    print(f"    â© å·²å­˜åœ¨ï¼ˆURLï¼‰ï¼š{full_url}")
                    if page_idx != start_page:
                        dup_streak += 1
                        print(f"    â†³ é€£çºŒé‡è¤‡ {dup_streak}/{DUP_STOP_THRESHOLD}")
                        if dup_streak >= DUP_STOP_THRESHOLD:
                            print("    â›” è§¸ç™¼ï¼šé€£çºŒ 5 å‰‡é‡è¤‡ï¼Œæº–å‚™è½ç›¤ä¸¦çµ‚æ­¢ã€‚")
                            atomic_save_json(out_file, all_articles)
                            print(f"  ğŸ’¾ å·²å¯«å…¥ï¼š{out_file}")
                            return
                    continue

                # é€²å…¥å…§é 
                driver.get(full_url)
                time.sleep(random.uniform(1.5, 3.5))

                article = extract_article_from_detail_html(
                    driver.page_source, full_url)
                if not article:
                    print(f"    âš ï¸ çµæ§‹ç¼ºå¤±ï¼Œç•¥éï¼š{full_url}")
                    # çµæ§‹ç¼ºå¤±ä¸æ˜¯é‡è¤‡ï¼Œä¸ç´¯è¨ˆ dup_streak
                    continue

                # Title å»é‡ï¼ˆä¿å®ˆï¼‰
                if article.title in seen_titles:
                    print(f"    â© å·²å­˜åœ¨ï¼ˆTitleï¼‰ï¼š{article.title}")
                    if page_idx != start_page:
                        dup_streak += 1
                        print(f"    â†³ é€£çºŒé‡è¤‡ {dup_streak}/{DUP_STOP_THRESHOLD}")
                        if dup_streak >= DUP_STOP_THRESHOLD:
                            print("    â›” è§¸ç™¼ï¼šé€£çºŒ 5 å‰‡é‡è¤‡ï¼Œæº–å‚™è½ç›¤ä¸¦çµ‚æ­¢ã€‚")
                            atomic_save_json(out_file, all_articles)
                            print(f"  ğŸ’¾ å·²å¯«å…¥ï¼š{out_file}")
                            return
                    continue

                # å¯«å…¥æ–°è³‡æ–™
                all_articles.append(asdict(article))
                seen_urls.add(full_url)
                seen_titles.add(article.title)
                dup_streak = 0  # æ–°å¢æˆåŠŸï¼Œé‡ç½®é€£çºŒé‡è¤‡è¨ˆæ•¸

                # âœ… å³æ™‚ä¿å­˜ï¼ˆåŸå­å¯«å…¥ï¼‰
                atomic_save_json(out_file, all_articles)

                print(
                    f"    âœ… {article.date} | {article.title[:30]}..."
                    f" ï¼ˆç¬¬ {idx}/{len(links)} å‰‡ï¼‰"
                )

    finally:
        driver.quit()

    end_time = datetime.now()
    print(f"çµæŸæ™‚é–“: {end_time:%Y-%m-%d %H:%M:%S}")
    elapsed = (end_time - start_time).total_seconds()
    print(f"ç¨‹å¼è€—æ™‚: {elapsed:.1f} ç§’")
    print(f"ç¸½è¨ˆç´¯ç©ï¼š{len(all_articles)} å‰‡")

# -------------------------- å…¥å£é» --------------------------


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="EY æ–°èçˆ¬èŸ²ï¼ˆå¢é‡å»é‡ã€å³æ™‚ä¿å­˜ã€é€£çºŒé‡è¤‡çµ‚æ­¢ï¼‰"
    )
    parser.add_argument(
        "--start-page",
        type=int,
        default=1,
        help="èµ·å§‹é ï¼ˆèµ·å§‹é ä¸å¥—ç”¨é€£çºŒé‡è¤‡çµ‚æ­¢è¦å‰‡ï¼‰",
    )
    parser.add_argument(
        "--max-page",
        type=int,
        default=MAX_PAGE,
        help=f"æœ€å¤§é ï¼Œé è¨­ {MAX_PAGE}",
    )
    args = parser.parse_args()
    if args.start_page < 1:
        raise ValueError("--start-page éœ€ç‚º >= 1 çš„æ•´æ•¸")
    if args.max_page < args.start_page:
        raise ValueError("--max-page ä¸å¯å°æ–¼ --start-page")
    return args


if __name__ == "__main__":
    cli = parse_args()
    crawl(start_page=cli.start_page, max_page=cli.max_page)
