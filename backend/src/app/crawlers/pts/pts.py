"""
PTSï¼ˆå…¬è¦–ï¼‰æ–‡ç« çˆ¬èŸ²ï¼šå¢é‡æ›´æ–° + é æœ«å³æ™‚å­˜æª” + é€£çºŒé‡è¤‡çµ‚æ­¢ + CLI èµ·å§‹é 
======================================================================

åŠŸèƒ½ç¸½è¦½
--------
1) æ ¹åˆ—è¡¨é ï¼š https://news.pts.org.tw/category/1
   å¯¦éš›æŠ“å–é ï¼š https://news.pts.org.tw/category/1?page={n}
2) é è¨­ç¿»é ç¯„åœ 1..100ï¼Œå¯ç”¨ CLI æŒ‡å®šèµ·å§‹é ï¼ˆ--startï¼‰èˆ‡ä¸Šé™ï¼ˆ--maxï¼‰ã€‚
3) å¢é‡æ›´æ–°ï¼š
   - è®€å– backend/data/raw/news/pts.json ä½œç‚ºèˆŠè³‡æ–™ï¼ˆè‹¥å­˜åœ¨ï¼‰
   - æ–°æŠ“è³‡æ–™èˆ‡èˆŠè³‡æ–™ä»¥ id å»é‡åˆä½µ
   - æ¯ã€Œé ã€è™•ç†çµæŸç«‹å³å¯«å› backend/data/raw/news/pts.jsonï¼ˆé¿å…ç•¶æ©Ÿå…¨å¤±ï¼‰
4) æ—©åœæ¢ä»¶ï¼š
   - é€£çºŒ 3 å‰‡ã€Œé‡è¤‡æ–°èã€ï¼ˆä»¥ URL è¡ç”Ÿä¹‹ id åˆ¤å®šï¼‰â†’ ç«‹å³çµ‚æ­¢æ•´é«”çˆ¬å–
   - è§£æå¤±æ•—ä¸ç®—é‡è¤‡ï¼Œæœƒä¸­æ–·ã€Œé€£çºŒã€è¨ˆæ•¸
   
5) æŒ‡ä»¤åˆ—åƒæ•¸ï¼š
   - ç”±ç¬¬ 1 é é–‹å§‹ï¼Œæœ€å¤šåˆ°ç¬¬ 100 é  $python src/knowledge_base_operation/news_crawler/pts/pts.py
   - å¾ç¬¬ 37 é é–‹å§‹ï¼Œæœ€å¤šåˆ°ç¬¬ 100 é  $python src/knowledge_base_operation/news_crawler/pts/pts.py --start 37
   - å¾ç¬¬ 20 é é–‹å§‹åˆ°ç¬¬ 60 é  $python src/knowledge_base_operation/news_crawler/pts/pts.py --start 20 --max 60

è³‡æ–™æ¬„ä½
--------
date, publisher, category, title, content, label, url, id
"""

from __future__ import annotations

import argparse
import json
import re
import time
from datetime import datetime
from hashlib import md5
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Set, Tuple
from zoneinfo import ZoneInfo

import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¸¸æ•¸èˆ‡è·¯å¾‘è¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BASE_CATEGORY_URL: str = "https://news.pts.org.tw/category/1"
PAGED_CATEGORY_URL: str = "https://news.pts.org.tw/category/1?page={}"

# é è¨­æœ€å¤§ç¿»é ä¸Šé™ï¼ˆéœ€æ±‚æŒ‡å®šåˆ° 100ï¼‰
DEFAULT_MAX_PAGES: int = 100

# å›ºå®šè¼¸å‡ºï¼ˆå¢é‡åˆä½µå¾Œè¦†å¯«å¯«å…¥ï¼Œä½†è³‡æ–™ç‚ºç´¯ç©çµæœï¼‰
OUTPUT_DIR: Path = Path("backend/data/raw/news")
OUTPUT_FILE: Path = OUTPUT_DIR / "pts.json"

# è«‹æ±‚æ¨™é ­ï¼šæ¨¡æ“¬å¸¸è¦‹ç€è¦½å™¨ï¼Œé™ä½è¢«æ“‹é¢¨éšª
HEADERS: Dict[str, str] = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9",
    "Connection": "keep-alive",
}

# è‹¥éœ€å•Ÿç”¨å…§å®¹éæ¿¾å¯è¨­å®šé—œéµå­—ï¼ˆç©ºå­—ä¸²ä»£è¡¨ä¸éæ¿¾ï¼‰
KEYWORD: str = ""

# å°åŒ—æ™‚å€ï¼ˆé¡¯ç¤ºç”¨ï¼‰
TPE_TZ: ZoneInfo = ZoneInfo("Asia/Taipei")

# æ–‡å­—æ¸…ç†ç”¨æ­£å‰‡ï¼šç§»é™¤å¤šé¤˜ç¬¦è™Ÿèˆ‡é€£çºŒç©ºç™½
_CLEAN_PAT = re.compile(r'[\n"ã€Œã€ï¼šï¼›:;,{}\[\]]|\s{2,}', flags=re.UNICODE)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åŸºç¤å·¥å…·
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(text: str) -> str:
    """
    å°‡æ®µè½ä¸­çš„å¤šé¤˜ç¬¦è™Ÿã€é€£çºŒç©ºç™½ç§»é™¤ä¸¦ä¿®å‰ªé¦–å°¾ç©ºç™½ã€‚

    è¨­è¨ˆç†ç”±ï¼š
    - åŸå§‹ HTML æ–‡å­—å¸¸æœ‰æ›è¡Œèˆ‡ä¸­è‹±æ–‡æ¨™é»åƒé›œï¼Œå…ˆè¡Œæ­£è¦åŒ–å¯æå‡ä¸€è‡´æ€§ã€‚
    """
    return _CLEAN_PAT.sub(" ", text).strip()


def parse_date(date_text: str) -> str:
    """
    å˜—è©¦å°‡å¤šç¨®æ ¼å¼çš„æ—¥æœŸå­—ä¸²è½‰ç‚º YYYY-MM-DDï¼›å¤±æ•—æ™‚å›å‚³ç©ºå­—ä¸²ã€‚

    æ”¯æ´çš„å¸¸è¦‹æ ¼å¼ï¼ˆå¯æŒ‰éœ€æ“´å……ï¼‰ï¼š
    - 2025/08/11, 2025-08-11, 2025å¹´08æœˆ11æ—¥, 2025.08.11
    - å«æ™‚é–“è€…æœƒåƒ…å–æ—¥æœŸéƒ¨åˆ†ï¼ˆä¾‹å¦‚ 2025/08/11 12:30ï¼‰
    """
    date_text = clean_text(date_text)
    date_formats: Tuple[str, ...] = (
        "%Y/%m/%d",
        "%Y-%m-%d",
        "%Yå¹´%mæœˆ%dæ—¥",
        "%Y.%m.%d",
        "%Y/%m/%d %H:%M",
        "%Y-%m-%d %H:%M",
    )
    for fmt in date_formats:
        try:
            cleaned = date_text.split(" ")[0]
            dt_obj = datetime.strptime(cleaned, fmt)
            return dt_obj.strftime("%Y-%m-%d")
        except ValueError:
            continue
    return ""


def make_article_id(url: str) -> str:
    """
    ç”± URL è¡ç”Ÿç©©å®šçŸ­ IDï¼š'PTS_' + md5(url) å‰ 8 ç¢¼ã€‚

    ç†å¿µï¼š
    - åŒä¸€ç¯‡æ–‡ç« çš„ URL æ‡‰è©²å”¯ä¸€ï¼Œå› æ­¤ä»¥å…¶ä½œç‚ºä¸»éµå¯å¤©ç„¶å»é‡ã€‚
    """
    return f"PTS_{md5(url.encode('utf-8-sig')).hexdigest()[:8]}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HTTP èˆ‡ HTML è§£æ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_html(url: str, session: requests.Session) -> Optional[str]:
    """
    ä»¥ GET å–å¾— HTMLï¼›æˆåŠŸå›å‚³å­—ä¸²ï¼Œå¤±æ•—å›å‚³ Noneã€‚

    é¢¨éšªæ§ç®¡ï¼š
    - è¨­å®š timeout é¿å…é•·æ™‚é–“æ›ä½ã€‚
    - é 200 ç‹€æ…‹ç¢¼ç›´æ¥è¦–ç‚ºå¤±æ•—ï¼Œä¸å˜—è©¦è§£æéŒ¯èª¤é ã€‚
    """
    try:
        res = session.get(url, headers=HEADERS, timeout=10)
        if res.status_code == 200:
            res.encoding = "utf-8-sig"
            return res.text
        print(f"âŒ HTTP {res.status_code} â†’ {url}")
    except requests.RequestException as exc:
        print(f"âŒ è«‹æ±‚éŒ¯èª¤ï¼š{exc} â†’ {url}")
    return None


def extract_links(list_html: str) -> List[str]:
    """
    å¾åˆ†é¡åˆ—è¡¨é  HTML æ“·å–æ‰€æœ‰æ–‡ç« é€£çµã€‚

    é¸æ“‡å™¨ç­–ç•¥ï¼š
    - ä½¿ç”¨ 'h2 a[href*="/article/"]'
    - ç›¸å°è·¯å¾‘è£œä¸Š https://news.pts.org.tw
    """
    soup = BeautifulSoup(list_html, "html.parser")
    anchors = soup.select('h2 a[href*="/article/"]')
    links: List[str] = []
    for a in anchors:
        href = a.get("href") or ""
        if not href:
            continue
        if href.startswith("/article/"):
            links.append("https://news.pts.org.tw" + href)
        elif href.startswith("https://news.pts.org.tw/article/"):
            links.append(href)
    return links


def parse_article(url: str, session: requests.Session) -> Optional[Dict]:
    """
    è§£æå–®ç¯‡æ–°èé é¢ï¼Œå›å‚³çµæ§‹åŒ–è³‡æ–™ï¼›å¤±æ•—å›å‚³ Noneã€‚

    èƒå–æ¬„ä½èªªæ˜ï¼š
    - titleï¼šå„ªå…ˆ 'h1.article-title'ï¼Œé€€è€Œæ±‚å…¶æ¬¡ 'h1'
    - dateï¼šå˜—è©¦ 'time' æˆ– 'span.date'ï¼Œå¤šæ ¼å¼è§£æ
    - categoryï¼šå˜—è©¦è‡ª 'div.news-info' æ–‡å­—æ¨æ–·ï¼ˆå¸¸è¦‹æ ¼å¼å«ç™¼å¸ƒè€…ï½œåˆ†é¡ï¼‰
    - contentï¼šè‡ª 'div.post-article.text-align-left' å–å…¨æ–‡ï¼Œä¾ 'ã€‚' åˆ‡å¥å¾Œé‡çµ„
    """
    html = fetch_html(url, session)
    if not html:
        return None

    soup = BeautifulSoup(html, "html.parser")

    # æ¨™é¡Œ
    title_tag = soup.select_one("h1.article-title") or soup.select_one("h1")
    title = clean_text(title_tag.text) if title_tag else ""
    if not title:
        print("âš ï¸ ç„¡æ¨™é¡Œï¼Œè·³é")
        return None

    # æ—¥æœŸ
    date_tag = soup.select_one("time") or soup.select_one("span.date")
    date_raw = date_tag.text if date_tag else ""
    date_str = parse_date(date_raw)

    # åˆ†é¡ï¼ˆå¾è³‡è¨Šåˆ—æ¨æ–·ï¼‰
    category = "æœªçŸ¥åˆ†é¡"
    info_div = soup.select_one("div.news-info")
    if info_div:
        all_txt = clean_text(info_div.get_text())
        if "|" in all_txt:
            parts = [p.strip() for p in all_txt.split("|") if p.strip()]
            if len(parts) >= 2:
                category = parts[1]
            elif parts:
                category = parts[0]
        elif all_txt:
            category = all_txt

    # å…§æ–‡
    content_div = soup.select_one("div.post-article.text-align-left")
    content_raw = clean_text(content_div.get_text()) if content_div else ""
    sentences = [s.strip() for s in content_raw.split("ã€‚") if s.strip()]
    content = "\n".join(f"{s}ã€‚" for s in sentences)

    item = {
        "date": date_str,
        "publisher": "pts",
        "category": category,
        "title": title,
        "content": content,
        "label": True,
        "url": url,
        "id": make_article_id(url),
    }

    # å¯é¸çš„é—œéµå­—éæ¿¾ï¼ˆKEYWORD éç©ºæ™‚å•Ÿç”¨ï¼‰
    if KEYWORD and (KEYWORD not in (item["title"] + item["content"])):
        return None

    return item


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¢é‡æ›´æ–°ï¼šè®€èˆŠæª”ã€åˆä½µå»é‡ã€å¯«å›
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_existing_records(file_path: Path) -> List[Dict]:
    """
    è®€å–æ—¢æœ‰çš„ pts_news.jsonï¼›è‹¥ä¸å­˜åœ¨æˆ–æ ¼å¼éŒ¯èª¤ï¼Œå›å‚³ç©ºæ¸…å–®ã€‚
    """
    if not file_path.exists():
        return []
    try:
        with file_path.open("r", encoding="utf-8-sig") as fp:
            data = json.load(fp)
            return data if isinstance(data, list) else []
    except (OSError, json.JSONDecodeError) as exc:
        print(f"âš ï¸ èˆŠæª”è®€å–å¤±æ•—ï¼š{exc} â†’ {file_path}")
        return []


def merge_deduplicate(
    old_items: Iterable[Dict],
    new_items: Iterable[Dict],
) -> List[Dict]:
    """
    å°‡èˆŠè³‡æ–™èˆ‡æ–°è³‡æ–™åˆä½µä¸¦å»é‡ï¼›åŒ ID æ™‚ã€Œä¿ç•™èˆŠè³‡æ–™ç‰ˆæœ¬ã€ã€‚

    ç†å¿µï¼š
    - èˆŠè³‡æ–™å¯èƒ½å·²ç¶“éäººå·¥ä¿®è¨‚æˆ–æ¸…æ´—ï¼Œä¿ç•™èˆŠç‰ˆæœ¬è¼ƒç‚ºç©©å¥ã€‚
    """
    merged: List[Dict] = []
    seen: Set[str] = set()

    # å…ˆæ”¾èˆŠè³‡æ–™
    for it in old_items:
        _id = it.get("id")
        if not _id or _id in seen:
            continue
        merged.append(it)
        seen.add(_id)

    # å†è£œå…¥æ–°è³‡æ–™ä¸­å°šæœªå‡ºç¾çš„
    for it in new_items:
        _id = it.get("id")
        if not _id or _id in seen:
            continue
        merged.append(it)
        seen.add(_id)

    return merged


def atomic_save_json(file_path: Path, data: List[Dict]) -> None:
    """
    æ¡ç”¨ã€Œå…ˆå¯«è‡¨æ™‚æª” â†’ åŸå­è½‰åã€çš„æ–¹å¼å¯«å…¥ JSONï¼Œé™ä½ç•¶æ©Ÿæ™‚æª”æ¡ˆæ¯€æé¢¨éšªã€‚
    """
    file_path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = file_path.with_suffix(file_path.suffix + ".tmp")
    with tmp_path.open("w", encoding="utf-8-sig") as fp:
        json.dump(data, fp, ensure_ascii=False, indent=2)
    tmp_path.replace(file_path)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»æµç¨‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_pts(start_page: int, max_pages: int) -> None:
    """
    PTS çˆ¬èŸ²é€²å…¥é»ã€‚

    å…·é«”æµç¨‹ï¼š
    1) è®€å–æ—¢æœ‰æª”æ¡ˆï¼ˆè‹¥å­˜åœ¨ï¼‰â†’ å–å¾—èˆŠè³‡æ–™èˆ‡å…¶ ID é›†åˆ
    2) è‡ª page=start_page èµ·ä¾åºæŠ“å–è‡³ page=max_pages
    3) é€é€£çµï¼š
       - å…ˆä»¥ URL è¨ˆç®— idï¼Œè‹¥ id å·²å­˜åœ¨ï¼ˆèˆŠæˆ–æœ¬è¼ªå·²æ–°å¢ï¼‰â†’ è¨˜ä¸€æ¬¡ã€Œé‡è¤‡ã€
       - è‹¥ã€Œé€£çºŒé‡è¤‡ã€é” 3 æ¬¡ â†’ è¦–ç‚ºå·²ç„¡æ–°æ–™ï¼Œç«‹å³çµ‚æ­¢æ•´é«”æµç¨‹
       - å¦å‰‡è§£ææ–‡ç« ä¸¦åŠ å…¥çµæœé›†ï¼›è¨ˆæ•¸é‡ç½®
    4) **æ¯é çµæŸ**å³èˆ‡èˆŠè³‡æ–™åˆä½µå»é‡ä¸¦å¯«å›å›ºå®šæª”æ¡ˆï¼ˆé¿å…ç•¶æ©Ÿå…¨å¤±ï¼‰
    """
    if start_page < 1:
        print(f"âš ï¸ èµ·å§‹é  {start_page} ç„¡æ•ˆï¼Œæ”¹ç”¨ 1")
        start_page = 1
    if max_pages < start_page:
        print(f"âš ï¸ max_pages({max_pages}) < start_page({start_page})ï¼Œä¸åŸ·è¡Œ")
        return

    start = datetime.now(TPE_TZ)
    print("â–¶ é–‹å§‹ï¼š", start.strftime("%Y-%m-%d %H:%M:%S"))
    print("â—† æ ¹åˆ—è¡¨é ï¼š", BASE_CATEGORY_URL)
    print(f"â—† ç¿»é ç¯„åœï¼š{start_page}..{max_pages}ï¼ˆæˆ–é€£çºŒ 3 é‡è¤‡å³æå‰çµ‚æ­¢ï¼‰")

    # 1) è®€å–èˆŠè³‡æ–™ â†’ è½‰æˆä»¥ id ç‚ºéµçš„å­—å…¸ä»¥åˆ©å¿«é€Ÿåˆ¤é‡èˆ‡ç´¯ç©
    existing_list = load_existing_records(OUTPUT_FILE)
    records_by_id: Dict[str, Dict] = {}
    for it in existing_list:
        _id = it.get("id")
        if _id and _id not in records_by_id:
            records_by_id[_id] = it

    print(f"ğŸ“¦ èˆŠè³‡æ–™ç­†æ•¸ï¼š{len(records_by_id)}")

    session = requests.Session()
    consecutive_dups = 0
    stop = False

    # 2) é€é æŠ“å–
    for page in range(start_page, max_pages + 1):
        if stop:
            break

        page_url = PAGED_CATEGORY_URL.format(page)
        print(f"\nğŸ” è§£æç¬¬ {page} é  â†’ {page_url}")

        list_html = fetch_html(page_url, session)
        if not list_html:
            print("âš ï¸ åˆ—è¡¨é æŠ“å–å¤±æ•—ï¼Œè·³éæ­¤é ")
            # å³ä¾¿æ­¤é å¤±æ•—ï¼Œä¹Ÿåœ¨é æœ«ä¿å­˜ç›®å‰ç´¯ç©ï¼ˆç¶­æŒã€Œæ¯é çµæŸå°±å­˜ã€ç²¾ç¥ï¼‰
            atomic_save_json(OUTPUT_FILE, list(records_by_id.values()))
            continue

        links = extract_links(list_html)
        print(f"âœ… å–å¾—é€£çµæ•¸ï¼š{len(links)}")

        # é€é€£çµè™•ç†
        for idx, article_url in enumerate(links, start=1):
            art_id = make_article_id(article_url)

            # å…ˆä»¥ id åˆ¤é‡ï¼ˆé¿å…ç„¡è¬‚è«‹æ±‚ï¼‰
            if art_id in records_by_id:
                consecutive_dups += 1
                print(
                    f"  â†ªï¸ ({idx}/{len(links)}) é‡è¤‡ï¼ˆ{consecutive_dups}/3ï¼‰ï¼š"
                    f"{article_url}"
                )
                if consecutive_dups >= 3:
                    print("ğŸ›‘ é€£çºŒ 3 å‰‡é‡è¤‡ï¼Œçµ‚æ­¢çˆ¬å–")
                    stop = True
                    break
                continue

            # ç¢ºèªç‚ºæ–°æ–‡ç«  â†’ è§£æå…§å®¹
            item = parse_article(article_url, session)
            if not item:
                # è§£æå¤±æ•—ä¸ç®—é‡è¤‡ï¼Œé‡ç½®é€£çºŒé‡è¤‡è¨ˆæ•¸
                consecutive_dups = 0
                continue

            # é—œéµå­—éæ¿¾è‹¥å•Ÿç”¨ï¼Œparse_article å·²è™•ç†è¿”å› None
            records_by_id[item["id"]] = item

            # æ–°æ–‡ç« å‡ºç¾ â†’ é€£çºŒé‡è¤‡æ­¸é›¶
            consecutive_dups = 0

            print(f"  âœ ({idx}/{len(links)}) æ–°å¢ï¼š{item['title']}")
            time.sleep(2)  # ç¦®è²Œæ€§å»¶é²ï¼Œé™ä½å°æ–¹ä¼ºæœè² è¼‰

        # 3) é æœ«ç«‹å³å­˜æª”ï¼ˆå³ä¾¿æœ¬é å®Œå…¨æ²’æ–°å¢ä¹Ÿæœƒè¦†å¯«ä¸€æ¬¡ï¼Œç¢ºä¿ç‹€æ…‹ä¸€è‡´ï¼‰
        atomic_save_json(OUTPUT_FILE, list(records_by_id.values()))
        print(f"ğŸ’¾ å·²å­˜æª”ï¼š{OUTPUT_FILE.resolve()}ï¼ˆç´¯è¨ˆ {len(records_by_id)} ç­†ï¼‰")

    end = datetime.now(TPE_TZ)
    print("\nğŸ‰ å®Œæˆ")
    print("ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š", OUTPUT_FILE.resolve())
    print("ğŸ“Š æœ€çµ‚ç¸½ç­†æ•¸ï¼š", len(records_by_id))
    print("â±ï¸ è€—æ™‚ï¼ˆç§’ï¼‰ï¼š", f"{(end - start).total_seconds():.1f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI å…¥å£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_args() -> argparse.Namespace:
    """
    è§£æå‘½ä»¤åˆ—åƒæ•¸ã€‚
    - --start / -sï¼šæŒ‡å®šèµ·å§‹é ï¼ˆé è¨­ 1ï¼‰
    - --max / -mï¼šæŒ‡å®šæœ€å¤§é ï¼ˆé è¨­ 100ï¼‰
    """
    parser = argparse.ArgumentParser(
        description="PTSï¼ˆå…¬è¦–ï¼‰åˆ†é¡æ–°èçˆ¬èŸ²ï¼ˆå¢é‡æ›´æ–°ï¼é æœ«å³æ™‚å­˜æª”ï¼é€£çºŒé‡è¤‡æ—©åœï¼‰"
    )
    parser.add_argument(
        "--start",
        "-s",
        type=int,
        default=1,
        help="èµ·å§‹é ï¼ˆé è¨­ï¼š1ï¼‰",
    )
    parser.add_argument(
        "--max",
        "-m",
        type=int,
        default=DEFAULT_MAX_PAGES,
        help=f"æœ€å¤§é ï¼ˆé è¨­ï¼š{DEFAULT_MAX_PAGES}ï¼‰",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    scrape_pts(start_page=args.start, max_pages=args.max)
