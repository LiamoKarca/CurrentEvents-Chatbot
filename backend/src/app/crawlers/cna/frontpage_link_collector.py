import json
import random
import re
import time
from pathlib import Path

from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC

from selenium_config import driver, wait

# ====== åƒæ•¸è¨­ç½® ======
URL_LIST_PAGE = "https://www.cna.com.tw/list/aipl.aspx"
VIEW_MORE_BTN = "#SiteContent_uiViewMoreBtn_Style3"
NEWS_LIST_ITEM = "#jsMainList > li"
TARGET_NUM = 9999  # å¯è¦–å°ˆæ¡ˆä¸Šé™èª¿æ•´
OUT_PATH = Path("backend/data/raw/news/cna_list.json")
OUT_PATH.parent.mkdir(parents=True, exist_ok=True)

# æ–°èæ—¥æœŸæ ¼å¼é©—è­‰ï¼ˆ2025/08/01 13:11ï¼‰
DATE_PATTERN = re.compile(r"^\d{4}/\d{2}/\d{2} \d{2}:\d{2}$")


def is_strict_valid(item: dict) -> bool:
    """æª¢æŸ¥æ¨™é¡Œã€æ—¥æœŸã€é€£çµçš†åˆæ³•ã€‚"""
    if not item:
        return False
    title = item.get("title", "").strip()
    date = item.get("date", "").strip()
    link = item.get("link", "").strip()
    if not (title and date and link):
        return False
    if not link.startswith("https://www.cna.com.tw/news/aipl/"):
        return False
    if not DATE_PATTERN.match(date):
        return False
    return True


def _extract_items():
    """åƒ…æ”¶éŒ„ç¬¦åˆæ¨™æº–çš„ liã€‚"""
    items, results = driver.find_elements(By.CSS_SELECTOR, NEWS_LIST_ITEM), []
    for li in items:
        try:
            a_tag = li.find_element(By.CSS_SELECTOR, "a")
            link = a_tag.get_attribute("href").strip() if a_tag else ""
            title = ""
            date = ""
            try:
                title_span = li.find_element(
                    By.CSS_SELECTOR, "div.listInfo h2 span")
                title = title_span.text.strip()
            except Exception:
                pass
            try:
                date_div = li.find_element(By.CSS_SELECTOR, "div.listInfo div")
                date = date_div.text.strip()
            except Exception:
                pass

            item = {"title": title, "date": date, "link": link}
            if is_strict_valid(item):
                results.append(item)
        except Exception:
            pass
    return results


def _click_view_more():
    """é»æ“ŠæŸ¥çœ‹æ›´å¤šï¼ˆè‹¥æœ‰ï¼‰ï¼Œè‹¥ç„¡å‰‡ç›´æ¥ç•¥éã€‚"""
    try:
        btn = wait.until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, VIEW_MORE_BTN))
        )
        driver.execute_script("arguments[0].click();", btn)
        return True
    except Exception:
        return False


def _scroll_down(body):
    """æ¨¡æ“¬ PAGE_DOWNï¼Œç•¥åŠ éš¨æ©Ÿç­‰å¾…ã€‚"""
    body.send_keys(Keys.PAGE_DOWN)
    time.sleep(random.uniform(0.25, 0.45))


def collect_cna_ai_links():
    driver.get(URL_LIST_PAGE)
    body = driver.find_element(By.TAG_NAME, "body")
    collected = {}
    last_update = time.time()

    while len(collected) < TARGET_NUM:

        _click_view_more()
        _scroll_down(body)

        items = _extract_items()

        for item in items:
            if item["link"] not in collected:
                collected[item["link"]] = item
                last_update = time.time()
                print(f"â‡¢ æ”¶éŒ„ï¼š{item['title']} ({item['date']})")

        if time.time() - last_update >= 12:
            print("âš ï¸  12 ç§’å…§ç„¡æ–°è³‡æ–™ï¼Œå¼·åˆ¶çµæŸã€‚")
            break

    return list(collected.values())


if __name__ == "__main__":
    try:
        # ========== å¢é‡å„²å­˜å€å¡Š ==========
        # 1. è®€å–èˆŠ list.json
        if OUT_PATH.exists():
            with OUT_PATH.open("r", encoding="utf-8-sig") as fp:
                old_data = json.load(fp)
            print(f"ğŸ—‚ï¸  æª¢æ¸¬åˆ°èˆŠæœ‰æ–°è {len(old_data)} ç­†")
        else:
            old_data = []
            print("ğŸ—‚ï¸  æœªåµæ¸¬åˆ°èˆŠæª”ï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆã€‚")

        # 2. å»ºç«‹ link â†’ item å¿«å–ï¼ˆåƒ…æœ‰æ•ˆè³‡æ–™ï¼‰
        old_link_map = {
            item["link"]: item for item in old_data if is_strict_valid(item)}
        print(f"ğŸ” å·²æ•´ç†å‡º {len(old_link_map)} ç­†æœ‰æ•ˆèˆŠæ–°è")

        # 3. åŸ·è¡Œæ–°ä¸€è¼ªæ–°èçˆ¬å–
        strict_data = collect_cna_ai_links()
        print(f"âœ… æœ¬æ¬¡å…±æ”¶éŒ„ {len(strict_data)} å‰‡æ–°èã€‚")

        # 4. æ¯”å°æ–°å¢
        new_links = [item for item in strict_data if item["link"]
                     not in old_link_map]
        num_new = len(new_links)

        # 5. åˆä½µæ‰€æœ‰æ–°è
        for item in new_links:
            old_link_map[item["link"]] = item

        total = len(old_link_map)
        print(f"âš ï¸  èˆ‡èˆŠæœ‰æ–°èæ¯”å°å¾Œï¼Œæ–°å¢ {num_new} ç­†æ–°çš„æ–°èã€‚")
        print(
            f"ğŸ”¸ ç´¯ç©ç¸½æ•¸ (èˆŠæœ‰ {len(old_link_map)-num_new} ç­†) + (æ–°å¢ {num_new} ç­†) å…± {total} å‰‡ï¼Œå·²é€²è¡Œæ›´æ–°ä¸¦å¯«å…¥ {OUT_PATH.resolve()}")

        # 6. å¯«å›ï¼ˆæ–°åˆ°èˆŠæ’åºï¼‰
        all_news = sorted(
            old_link_map.values(),
            key=lambda x: x["date"],
            reverse=True
        )
        with OUT_PATH.open("w", encoding="utf-8-sig") as fp:
            json.dump(all_news, fp, ensure_ascii=False, indent=2)

    finally:
        driver.quit()
